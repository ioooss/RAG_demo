# -*- coding: utf-8 -*-
# File: prompts.py 

"""
该文件是一些项目实用prompts的集合，用于实现扩展任务，例如query改写，意图识别，知识规整，HyDE，StepBack, Multi-Query, 查询扩展，ReACT反思优化。
"""


def query_stepback(llm, query):
    """
    如果原始查询太复杂或返回的信息太广泛，我们可以选择生成一个抽象层次更高的“退后”问题，可以单独作为一路召回，也可以与原始问题一起用于检索，以增加返回结果的数量。
    例如，原问题是“丰田塞纳有几个轮子”，而退后问题可能是“汽车有几个轮子”。这种更高层次的问题可能更容易找到答案。 
    Args:
        llm: LLM模型实例，用于生成整理后的文档。
        query: 原始用户问题
    
    Returns:
        stepback_query: 概念回退后的问题
    """

    step_back_prompt = """
           策略：
           核心概念识环：首先确定问题的核心概念。例如，如果问题涉及到物理学中的力，那么可能需要后退到基础的力的定义和原理。
           你是世界知识的专家，擅长用后退提问策略，一步步仔细思考并回答问题。后退提问是一种思考策蹄，意在从更宏观或更基础的角度去理解和分析一个特定的问题或情境。
           这种策略要求我们在面对一个具体问题时，先"后退"一步，从一个更广泛或更根本的角度去提问和思考。这样做的目的是帮助我们更余入地理解问题的背景、原因或相关的基础知识，从而更好地回答原始问题。
           同题的范围：尝试识别问题的范困和上下文。这有助干确定后退的深度。有些问题可能只需要稍微后退一步，而其他问题可能需要深入到皇酣原理。
               历史和背景：对于一些问题，了解其历史背景和发展可能会有助于提出恰当的后退问题。
               原理和假设：明确当前问题的基础原理和假设。这可以帮助确定应该从哪些方面后退。
               格式：一个原始提问对应一个后退提问。
           例子：
           原始提问：截止2019年，英格兰队最新的一次进入世界杯四强是哪年？
           后退提问：英格兰截至2019年都在哪些年份进入了世界杯四强？
           原始提问：1993年11月28日，拉斯维加斯最豪华的酒店是哪家？
           后退提问：那时的拉斯维加斯，各大酒店的执棋如何
           原始提问：2017赛季，NBA中哪位球员的年薪最高？
           后退提问：在2017的NBA赛季，顶级球员的薪水大约是多少？
           原始提问：{query}
           后退提问: """

    stepback_query = llm.infer([prompt])[0]
    
    return stepback_query 



def multi_query(llm, query):
    """
    让LLM基于用户的query再生成多个查询语句，这些生成的查询语句是对用户查询语句的补充
    它们是从不同的视角来补充用户的查询语句，然后每条查询语句都会从向量数据库中检索到一批相关文档
    最后所有的相关文档都会被喂给LLM，这样LLM就会生成比较完整和全面的答案。
    这样就可以避免因为查询语句的差异而导致结果不正确。
    Args:
        llm: LLM模型实例，用于生成整理后的文档。
        query: 原始用户问题
    
    Returns:
        multi_query: query扩展后的查询问题 
    """

    expansion_prompt = f"""
       您是一位专业的助手，任务是将用户提出的问题转化为3个不同的问题表述。这些表述需要做到以下几点：
       核心概念覆盖：确保每个问题表述都紧密围绕原始问题的核心概念展开。
       多角度探讨：从技术挑战、应用场景、解决方案等多个角度来探索问题。
       相关性保证：每个问题表述都应与原始问题保持高度相关性，保持语义一致。
       创造性思考：鼓励创造性地思考问题，以激发新的见解和讨论。
       清晰表述：每个问题表述都应清晰、准确，易于理解。

       请根据以上指导原则，对用户的问题进行深入分析，
       原始提问：如何使用Python进行数据分析？
       扩充提问：1.如何使用Python进行数据分析？ 2.Python数据分析的最佳实践是什么？ 3.使用Python进行数据可视化有哪些推荐工具？
       原始提问：如何提高我的英语听力理解能力？
       扩充提问：1.有哪些有效的方法可以提升英语听力技巧？ 2.哪些在线资源可以帮助我提高英语听力？ 3.英语听力练习的最佳策略是什么？
       原始提问：推荐一款婴儿购物用品？
       扩充提问：1•寻找适合新生儿的便携式婴儿购物用品有哪些选项？ 2.哪些婴儿购物用品在安全性和便利性方面获得了家长的高评价？ 3.在为婴儿购物时，有哪些创新设计的产品可以提高购物体哈？
       原始提问：｛query｝。
       扩充提问："""

    query_expansion = llm.infer([expansion_prompt])[0]
    
    return query_expansion 



def clean_and_structure_documents(llm, documents):
    """
    使用LLM对文档进行整理，使其更加简洁和结构化。适用于候选文档的整理，冗余去重。
    
    Args:
        llm: LLM模型实例，用于生成整理后的文档。
        documents: 原始文档列表，每个文档是一个Document对象。
    
    Returns:
        cleaned_docs: 整理后的文档列表。
    """
    cleaned_docs = []
    for doc in documents:
        # 构造提示，要求LLM整理文档

        prompt = f"""
        你是一个专业的文档整理助手，负责对汽车用户手册中的内容进行整理和总结。请根据以下要求对文档进行处理：

        1. **去除冗余信息**：删除重复的句子、段落，保留核心内容。
        2. **结构化输出**：将文档内容按照逻辑顺序进行整理，确保内容连贯。
        3. **提取关键信息**：提取文档中的关键步骤、注意事项、操作说明等重要信息。
        4. **简洁表达**：确保整理后的文档简洁明了，避免冗长的描述。

        请根据以下文档内容进行整理：
        {doc}
        """

        # 使用LLM生成整理后的文档
        cleaned_content = llm.infer([prompt])[0]
        cleaned_docs.append(cleaned_content)
    
    return cleaned_docs



def generate_HyDE_query(llm, query):
    """
    HyDE查询，让通用LLM模型根据你的query生成一个可能的假想答案。
    用法：可以将这个假想答案去召回。想比原始多一路召回。即将假想答案、检索到的真实内容和原始问题一起输入到AI模型中，生成一个更加准确的回答。

    Args:
        llm: LLM模型实例。
        query: 用户的问题。

    Returns:
        initial_answer: 生成的初步答案。
    """

    fake_answer_prompt = f"""请根据以下问题生成一个初步答案。这个答案不需要非常详细，但需要尽可能与问题相关，提供一个简要的方向性回答。
                       *问题*
                       {query}

                       "任务"
                       1. 你的回答应尽量基于常识或通用知识。
                       2. 如果问题涉及具体领域知识，可以基于你的语言模型能力提供合理的猜测。
                       3. 请保持回答简洁明了。

                       *输出要求*
                       直接输出答案，如果没有合适的答案则输出换行符 \t
                       不要添加额外内容

                       输出："""

    initial_answer = llm.infer([fake_answer_prompt])[0]
    return initial_answer


def optimize_answer(llm, query, context, initial_answer, max_iterations=1):
    """
    使用LLM逐步优化生成的答案。这就是迭代优化，自我反思的思想，即ReACT。

    即我们可以把RAG系统输出的答案，上下文信息，以及当前输入再一次喂给大模型，让他反思和重新生成。
    
    Args:
        llm: LLM模型实例。
        query: 用户请求。
        context: 新的上下文信息。
        initial_answer: 系统初始生成的答案。
        max_iterations: 最大优化迭代次数。
    
    Returns:
        optimized_answer: 优化后的答案。
    """
    optimized_answer = initial_answer
    for i in range(max_iterations):
        # 构造优化提示
        prompt = f"""
        你是一个专业的问答助手，负责根据提供的上下文信息优化答案。请根据以下要求对答案进行优化：

        1. **结合上下文**：将以下上下文信息与当前答案结合，生成更准确的答案。
        2. **修正错误**：判断原始答案是否完美地回答了问题，如果当前答案有错误或不完整，请根据上下文进行反思和修正。
        3. **保持简洁**：确保优化后的答案简洁明了，避免冗长的描述。

        上下文信息：
        {context}

        用户问题：
        {query}

        当前答案：
        {optimized_answer}

        请生成优化后的答案：
        """
        
        # 调用LLM生成优化后的答案
        optimized_answer = llm.infer([prompt])[0]
    
    return optimized_answer


def generate_expanded_query(llm, query, use_expanded_query=True):
    """
    使用LLM生成扩展查询，以提高检索质量

    1. 让LLM直接回答问题生成初步答案
    2. 将原始问题和初步答案拼接成新的查询
    
    Args:
        llm: 大语言模型实例
        query: 原始查询
        use_expanded_query: 是否使用扩展查询功能
    
    Returns:
        expanded_query: 扩展后的查询
    """
    # 如果不使用扩展查询，直接返回原始查询
    if not use_expanded_query:
        logger.info(f"不使用扩展查询功能，使用原始查询: {query}")
        return query
    
    logger.info(f"为原始查询生成扩展查询: {query}")
    
    # 构造提示让LLM直接回答，不需要上下文
    direct_prompt = f"""请直接回答下面这个关于汽车的问题，简明扼要地提供可能的答案。如果不确定，可以提供一个合理的猜测：
    
    问题: {query}
    """
    
    # 使用LLM生成初步答案
    try:
        initial_answer = llm.infer([direct_prompt])[0]
        logger.info(f"LLM生成的初步答案: {initial_answer[:100]}...")
        
        # 将问题和初步答案拼接成扩展查询
        expanded_query = f"{query} {initial_answer}"
        logger.info(f"扩展查询生成完成，长度: {len(expanded_query)}")
        
        return expanded_query
    except Exception as e:
        logger.error(f"生成扩展查询时出错: {str(e)}")
        # 如果生成失败，返回原始查询
        return query


def organize_car_knowledge_with_llm(split_text):
    """
    使用LLM模型将杂乱的汽车知识资料整理规整。输入可以是被切分后的文本片段，可能会由于pdf解析或者或者切分原因导致信息的错乱，编排问题，用llm做一个规整。

    参数：
        llm: LLM模型实例。
        split_text (str): 杂乱的汽车知识资料字符串。

    返回：
        organized_text: 整理规整后的汽车知识资料字符串。
    """
    # 编写整理内容的 prompt
    prompt = f"""
    你是一个专业的汽车知识整理员。以下是一份杂乱的汽车知识资料，请你帮忙将其整理规整，使其条理清晰，结构明确：
    {split_text}

    要求：
    修改文本的语法错误和语义错误，使其语法正确，语义完整。请只给出修复整理后的文本，不要给出其他回复。

    整理后的内容如下：
    """

    organized_text = llm.infer([prompt])[0]
    return organized_text


def rag_docs_summary(llm, docs, max_length=4000):
    """
    对rerank后的top-k文档，进行提炼总结，生成一个文档摘要。这可以作为一个单独的文档分片一起输入给最后的大模型。
    参数：
        llm: LLM模型实例。
        docs (list[str]): top-k个候选文档（输入给最后大模型之前）

    返回：
        summary_doc: 总结的文档摘要
    """

    docs_str = simple_docs_concat(docs, max_length)
    prompt_template = ("以下是几段从知识库中检索出的文档，它们可能来自不同的来源，内容可能存在重复、冗余或逻辑混乱的问题。你的任务是：\n"
                       "1. 对这些文档的内容进行整合和归纳，总结出清晰的主题。\n"
                       "2. 根据主题提炼出文档的关键内容，并去掉重复的信息，对于内容相似的文档可以进行取舍与合并。\n"
                       "3. 将文档重新组织为多条逻辑清晰的文章，条理分明，语言简洁。\n\n"

                       "请按照以下格式输出：\n"
                       "---\n"
                       "1. [主题1。提炼出的关键内容]\n"
                       "2. [主题2。提炼出的关键内容]\n"
                       "3. [主题3。提炼出的关键内容]\n4. [...]\n"
                       "---\n\n"
                       "以下是需要整理的文档：\n{docs}".format(docs=docs_str))

    summary_doc = llm.infer([prompt_template])[0]
    return summary_doc 


def intent_recognition(llm, query):
    """
    query意图识别，判断是否用户输入是一个跟汽车相关的问题。一般用于系统的最开始，如果query跟汽车无关，则不进入系统，防止无意义或者闲聊的话进来。
    参数：
        llm: LLM模型实例。
        query (str): 用户的问题
    返回：
        str: 相关/无关。
    """

    allow_prompt = f'''
    *角色*
    你是一个专业的汽车智能助手，可以回答汽车及其系统、软件、服务、驾驶或交通、安全等相关的问题
    *任务*
    判断用户请求是否与汽车及其操作系统产品相关
    *输出要求*
    输出候选[相关, 无关]
    从候选中选择一个直接输出，不要添加额外内容
    ********
    用户请求：{query}
    ********
    输出：
    '''

    intent = llm.infer([allow_prompt])[0]
    return intent


def query_rewrite(llm, query, history):
    """
    用户query的改写，主要用于多轮对话。依据对话历史和当前输入，把当前信息不完整改写成完整的一句话。例如“雨刮器出现异响怎么维修-如何更换它” -> 改写为“如何更换雨刮器”
    参数：
        llm: LLM模型实例。
        query (str): 用户的问题
        history (list[str]): 用户的对话历史
    返回：
        rewrite_query: 改写后的query。
    """


    rewrite_prompt = f"""
    ## 角色
    请扮演一个资深的句式大师，从句子本身进行判断：当前句子是否需要对话历史的内容完成信息的补全或者改写，如果需要改写，则输出改写后的句子，否则输出“N”。

    ## 需要改写的包括了以下场景
    1. 当前句是上一轮输入句的主语，宾语或者其他重要信息的指代，例如“编导辅助有什么用-关闭它”；“北京大学什么时间成立的-去那里看一看””
    2. 当前输入句子句意的完整性不足，缺少谓语、宾语等关键信息，需要从历史内容中获取，则需要补全，例如“我的车有多长-宽度呢”
    3. 如果当前句子句意完整性不足，但通过历史输入能够补全、理解其具体意思。例如：“今天的天气怎么样-明天呢-后天呢”；“孕妇怎么系安全带-小孩子呢”等
    4. 如果上一轮输入句能给其补全一些额外的句子信息。例如：“成都的天气怎么样-明天呢-上海会下雨吗”，意思是上海明天会下雨吗
    5. 如果改写的内容可以推理输出更准确的结果，请直接输出准确结果，例如：“谢娜是哪里人-播放一首她老公的歌”，请直接输出“播放一首张杰的歌”

    ## 不需要改写的包括以下场景
    1. 如果上一轮输入句没有包含额外的句子信息。例如：“上海的天气怎么样-如何加玻璃水”。
    2. 句子陈述对象是车机助手，省略我、你等主语的情况应认为句意完整。例如：”（你）放首歌“、”（你）退下吧“等。
    5. 判断当前输入句子是不是一句有序正常的话，无序、乱序输入。

    ## 要求
    1. 输入句子可能是陈述句、疑问句、感叹句等多种句式，请根据不同句式特点判断其完整性
    2. 请经过慎重思考后，判断是否需要改写。记住：这个任务是做query的改写，不要回答问题，不要回答问题。
    3. 如果需要改写，则直接输出改写后的句子，记住不需要输出其他无关内容，只输出改写后的结果，不需要说分析理由。
    4. 如果综合判断后不需要改写，则输出“N”。
    
    对话历史：{history}
    当前输入：{query}

    改写后输出："""


    rewrite_query = llm.infer([rewrite_prompt])[0]

    if rewrite_query == "N":
        rewrite_query = query

    return rewrite_query
